#!/usr/bin/env python3
"""
Enhancement 1: Geographic Mapping and Moran's I Spatial Autocorrelation

Since exact SA1 centroids aren't readily available, this uses SA1 code structure
to approximate spatial relationships. SA1 codes encode geographic proximity -
neighboring areas have similar codes.

Approach:
- Build spatial weights matrix based on SA1 code similarity
- Calculate Global Moran's I for gentrification risk
- Calculate Local Moran's I (LISA) to identify hotspots and coldspots
- Classify areas into spatial clusters
"""

import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
from scipy.spatial.distance import cdist
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("ENHANCEMENT 1: SPATIAL AUTOCORRELATION ANALYSIS (MORAN'S I)")
print("=" * 80)
print()

# Load gentrification results
results_file = Path("gentrification_analysis_results/gentrification_risk_scores_all_sa1.csv")
df = pd.read_csv(results_file)

# Filter to residential areas with population > 0
df = df[df['total_population'] > 0].copy()
print(f"Loaded {len(df):,} residential SA1 areas")
print()

# ============================================================================
# SPATIAL WEIGHTS MATRIX CONSTRUCTION
# ============================================================================

print("STEP 1: Building spatial weights matrix based on SA1 code proximity...")
print("-" * 80)
print("Note: Using SA1 code structure to approximate geographic neighbors")
print("      SA1 codes encode state, region, and sub-region hierarchically")
print()

# Extract SA1 code components for spatial weighting
df['SA1_CODE'] = df['SA1_CODE_2021'].astype(str)

# SA1 code structure (11 digits):
# - Digit 1: State/Territory
# - Digits 2-3: SA4 region
# - Digits 4-6: SA3 region
# - Digits 7-9: SA2 region
# - Digits 10-11: SA1 identifier

# Create hierarchical proximity weights
df['state_code'] = df['SA1_CODE'].str[0]  # State
df['sa4_code'] = df['SA1_CODE'].str[0:3]  # SA4 region
df['sa3_code'] = df['SA1_CODE'].str[0:6]  # SA3 region
df['sa2_code'] = df['SA1_CODE'].str[0:9]  # SA2 region

# Map state codes to names
STATE_MAP = {'1': 'NSW', '2': 'VIC', '3': 'QLD', '4': 'SA',
             '5': 'WA', '6': 'TAS', '7': 'NT', '8': 'ACT', '9': 'OT'}
df['state'] = df['state_code'].map(STATE_MAP)

# For computational efficiency, limit to top 10,000 highest risk areas
# (Full analysis of 60k x 60k matrix would require massive memory)
print("For computational efficiency, analyzing top 10,000 highest risk areas...")
df_sample = df.nlargest(10000, 'gentrification_risk_score').copy().reset_index(drop=True)
print(f"Sample size: {len(df_sample):,} SA1 areas")
print()

n = len(df_sample)
y = df_sample['gentrification_risk_score'].values

# Build sparse spatial weights matrix
# Weight = 1 if areas share SA2 (same 9-digit prefix), 0 otherwise
print("Building spatial weights matrix...")
print("  - Neighbors defined as SA1 areas within same SA2 region")

# Create SA2 groups
sa2_groups = df_sample.groupby('sa2_code').groups

# Initialize spatial weights matrix (sparse representation)
W = np.zeros((n, n))

# Populate weights - areas in same SA2 are neighbors
for sa2, indices in sa2_groups.items():
    if len(indices) > 1:
        idx_list = list(indices)
        for i in idx_list:
            for j in idx_list:
                if i != j:
                    W[i, j] = 1

# Row-standardize weights (each row sums to 1)
row_sums = W.sum(axis=1)
row_sums[row_sums == 0] = 1  # Avoid division by zero
W_standardized = W / row_sums[:, np.newaxis]

# Count neighbors
neighbor_counts = (W > 0).sum(axis=1)
print(f"  âœ“ Spatial weights matrix built: {n}Ã—{n}")
print(f"  âœ“ Average neighbors per area: {neighbor_counts.mean():.1f}")
print(f"  âœ“ Areas with no neighbors: {(neighbor_counts == 0).sum()}")
print(f"  âœ“ Max neighbors: {neighbor_counts.max()}")
print()

# ============================================================================
# GLOBAL MORAN'S I
# ============================================================================

print("STEP 2: Calculating Global Moran's I...")
print("-" * 80)

# Calculate Global Moran's I
# I = (n/W) * Î£ Î£ w_ij * (y_i - È³)(y_j - È³) / Î£(y_i - È³)Â²

y_mean = y.mean()
y_deviation = y - y_mean

# Numerator: spatial covariance
numerator = 0
for i in range(n):
    for j in range(n):
        if W[i, j] > 0:
            numerator += W[i, j] * y_deviation[i] * y_deviation[j]

# Denominator: total variance
denominator = (y_deviation ** 2).sum()

# Total number of neighbors
W_sum = W.sum()

# Global Moran's I
if denominator > 0 and W_sum > 0:
    morans_i = (n / W_sum) * (numerator / denominator)
else:
    morans_i = 0

# Calculate expected value under null hypothesis (no spatial autocorrelation)
expected_i = -1 / (n - 1)

# Calculate variance and z-score (simplified)
# For proper inference, should account for spatial structure
variance_i = 1 / (n - 1)  # Simplified - actual calculation is complex
z_score = (morans_i - expected_i) / np.sqrt(variance_i)
p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))  # Two-tailed test

print(f"Global Moran's I: {morans_i:.4f}")
print(f"Expected I (null): {expected_i:.4f}")
print(f"Z-score: {z_score:.4f}")
print(f"P-value: {p_value:.6f}")
print()

if morans_i > 0:
    print("âœ“ POSITIVE SPATIAL AUTOCORRELATION DETECTED")
    print("  â†’ High-risk areas cluster together geographically")
    print("  â†’ Low-risk areas cluster together geographically")
elif morans_i < 0:
    print("âœ“ NEGATIVE SPATIAL AUTOCORRELATION DETECTED")
    print("  â†’ High-risk areas surrounded by low-risk areas (dispersion)")
else:
    print("âœ“ NO SPATIAL AUTOCORRELATION")
    print("  â†’ Random spatial distribution")
print()

# ============================================================================
# LOCAL MORAN'S I (LISA)
# ============================================================================

print("STEP 3: Calculating Local Moran's I (LISA) for each area...")
print("-" * 80)

# Local Moran's I for each location i:
# I_i = (y_i - È³) / ÏƒÂ² * Î£ w_ij * (y_j - È³)

variance_y = y_deviation.var()

# Calculate local I for each area
local_i = np.zeros(n)
for i in range(n):
    spatial_lag = 0
    for j in range(n):
        if W_standardized[i, j] > 0:
            spatial_lag += W_standardized[i, j] * y_deviation[j]

    local_i[i] = (y_deviation[i] / variance_y) * spatial_lag

# Classify areas into LISA categories
df_sample['local_morans_i'] = local_i

# Categorize into quadrants:
# HH (High-High): High value surrounded by high values (hotspot)
# LL (Low-Low): Low value surrounded by low values (coldspot)
# HL (High-Low): High value surrounded by low values (outlier)
# LH (Low-High): Low value surrounded by high values (outlier)

# Calculate spatial lag for each area
spatial_lag = W_standardized @ y

# Standardize values
y_std = (y - y_mean) / y.std()
lag_std = (spatial_lag - spatial_lag.mean()) / spatial_lag.std()

# Classify quadrants
def classify_lisa(y_val, lag_val, threshold=0):
    if y_val > threshold and lag_val > threshold:
        return 'HH (Hotspot)'
    elif y_val < threshold and lag_val < threshold:
        return 'LL (Coldspot)'
    elif y_val > threshold and lag_val < threshold:
        return 'HL (High-Low Outlier)'
    elif y_val < threshold and lag_val > threshold:
        return 'LH (Low-High Outlier)'
    else:
        return 'Not Significant'

df_sample['spatial_lag'] = spatial_lag
df_sample['lisa_category'] = [classify_lisa(y_std[i], lag_std[i])
                               for i in range(n)]

# Count LISA categories
lisa_counts = df_sample['lisa_category'].value_counts()
print("LISA Cluster Classification:")
print("-" * 80)
for category, count in lisa_counts.items():
    pct = (count / n) * 100
    print(f"  {category:25s}: {count:5,} ({pct:5.1f}%)")
print()

# ============================================================================
# IDENTIFY TOP SPATIAL CLUSTERS
# ============================================================================

print("STEP 4: Identifying top gentrification hotspots and coldspots...")
print("-" * 80)

# High-High hotspots (high gentrification risk surrounded by high risk)
hotspots = df_sample[df_sample['lisa_category'] == 'HH (Hotspot)'].copy()
hotspots = hotspots.nlargest(100, 'gentrification_risk_score')

print(f"\nTop 20 HOTSPOT Areas (High-High Clusters):")
print("-" * 80)
print("These are high-risk areas surrounded by other high-risk areas")
print()
display_cols = ['rank', 'SA1_CODE_2021', 'state', 'gentrification_risk_score',
                'local_morans_i', 'spatial_lag', 'total_population']
print(hotspots[display_cols].head(20).to_string(index=False))
print()

# Low-Low coldspots
coldspots = df_sample[df_sample['lisa_category'] == 'LL (Coldspot)'].copy()
coldspots = coldspots.nsmallest(100, 'gentrification_risk_score')

print(f"\nTop 20 COLDSPOT Areas (Low-Low Clusters):")
print("-" * 80)
print("These are low-risk areas surrounded by other low-risk areas")
print()
print(coldspots[display_cols].head(20).to_string(index=False))
print()

# ============================================================================
# STATE-LEVEL SPATIAL PATTERNS
# ============================================================================

print("STEP 5: State-level spatial autocorrelation patterns...")
print("-" * 80)

state_spatial = df_sample.groupby('state').agg({
    'local_morans_i': 'mean',
    'gentrification_risk_score': 'mean',
    'SA1_CODE_2021': 'count'
})
state_spatial.columns = ['Avg_Local_Morans_I', 'Avg_Risk_Score', 'SA1_Count']
state_spatial = state_spatial.sort_values('Avg_Local_Morans_I', ascending=False)

print(state_spatial.to_string())
print()

# Count LISA categories by state
lisa_by_state = pd.crosstab(df_sample['state'], df_sample['lisa_category'])
print("\nLISA Categories by State:")
print("-" * 80)
print(lisa_by_state.to_string())
print()

# ============================================================================
# EXPORT RESULTS
# ============================================================================

print("STEP 6: Exporting spatial analysis results...")
print("-" * 80)

output_dir = Path("gentrification_analysis_results")

# Export full spatial analysis
spatial_export_cols = ['rank', 'SA1_CODE_2021', 'state', 'gentrification_risk_score',
                       'local_morans_i', 'spatial_lag', 'lisa_category',
                       'total_population', 'median_personal_income', 'pct_year12',
                       'pct_young_professionals', 'edu_income_mismatch']

df_sample[spatial_export_cols].to_csv(output_dir / "spatial_analysis_top_10000.csv", index=False)
print(f"âœ“ Exported: {output_dir / 'spatial_analysis_top_10000.csv'}")

# Export hotspots
hotspots[spatial_export_cols].to_csv(output_dir / "spatial_hotspots_high_high.csv", index=False)
print(f"âœ“ Exported: {output_dir / 'spatial_hotspots_high_high.csv'}")

# Export coldspots
coldspots[spatial_export_cols].to_csv(output_dir / "spatial_coldspots_low_low.csv", index=False)
print(f"âœ“ Exported: {output_dir / 'spatial_coldspots_low_low.csv'}")

# Export state-level summary
state_spatial.to_csv(output_dir / "spatial_analysis_by_state.csv")
print(f"âœ“ Exported: {output_dir / 'spatial_analysis_by_state.csv'}")

# Export LISA by state
lisa_by_state.to_csv(output_dir / "lisa_categories_by_state.csv")
print(f"âœ“ Exported: {output_dir / 'lisa_categories_by_state.csv'}")

print()

# ============================================================================
# SUMMARY
# ============================================================================

print("=" * 80)
print("SPATIAL ANALYSIS COMPLETE")
print("=" * 80)
print()
print(f"Global Moran's I: {morans_i:.4f} (Z={z_score:.2f}, p={p_value:.6f})")
print()
if p_value < 0.05:
    print("âœ“ STATISTICALLY SIGNIFICANT spatial clustering detected")
else:
    print("âš  No statistically significant spatial clustering")
print()
print(f"Hotspots (HH): {len(hotspots):,} areas")
print(f"Coldspots (LL): {len(coldspots):,} areas")
print()
print("Interpretation:")
print("  â€¢ Hotspots = gentrification risk concentrated in geographic clusters")
print("  â€¢ These areas are likely to experience synchronized transformation")
print("  â€¢ Property investors should consider entire hotspot clusters, not just single SA1s")
print("  â€¢ Coldspots represent stable, low-gentrification regions")
print()
print("=" * 80)

# Sources
print("\nðŸ“š Data Sources:")
print("- [ABS Digital Boundary Files](https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files)")
print("- [ABS Statistical Geography](https://www.abs.gov.au/statistics/statistical-geography/using-statistical-geography)")
print("- [ASGS Edition 3 Boundaries](https://data.gov.au/data/dataset/asgs-edition-3-2021-boundaries)")
